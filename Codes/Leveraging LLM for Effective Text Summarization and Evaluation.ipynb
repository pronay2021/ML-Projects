{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6004344,"sourceType":"datasetVersion","datasetId":3438844}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-17T12:20:53.223222Z","iopub.execute_input":"2024-12-17T12:20:53.224058Z","iopub.status.idle":"2024-12-17T12:20:53.572183Z","shell.execute_reply.started":"2024-12-17T12:20:53.224021Z","shell.execute_reply":"2024-12-17T12:20:53.571301Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/samsum-dataset-text-summarization/samsum-train.csv\n/kaggle/input/samsum-dataset-text-summarization/samsum-test.csv\n/kaggle/input/samsum-dataset-text-summarization/samsum-validation.csv\n/kaggle/input/samsum-dataset-text-summarization/samsum_dataset/dataset_dict.json\n/kaggle/input/samsum-dataset-text-summarization/samsum_dataset/validation/state.json\n/kaggle/input/samsum-dataset-text-summarization/samsum_dataset/validation/dataset_info.json\n/kaggle/input/samsum-dataset-text-summarization/samsum_dataset/validation/data-00000-of-00001.arrow\n/kaggle/input/samsum-dataset-text-summarization/samsum_dataset/test/state.json\n/kaggle/input/samsum-dataset-text-summarization/samsum_dataset/test/dataset_info.json\n/kaggle/input/samsum-dataset-text-summarization/samsum_dataset/test/data-00000-of-00001.arrow\n/kaggle/input/samsum-dataset-text-summarization/samsum_dataset/train/state.json\n/kaggle/input/samsum-dataset-text-summarization/samsum_dataset/train/dataset_info.json\n/kaggle/input/samsum-dataset-text-summarization/samsum_dataset/train/data-00000-of-00001.arrow\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T12:21:13.424721Z","iopub.execute_input":"2024-12-17T12:21:13.425042Z","iopub.status.idle":"2024-12-17T12:21:21.522807Z","shell.execute_reply.started":"2024-12-17T12:21:13.425013Z","shell.execute_reply":"2024-12-17T12:21:21.521905Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\nfrom datasets import Dataset\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T12:21:21.524913Z","iopub.execute_input":"2024-12-17T12:21:21.525226Z","iopub.status.idle":"2024-12-17T12:21:39.028110Z","shell.execute_reply.started":"2024-12-17T12:21:21.525196Z","shell.execute_reply":"2024-12-17T12:21:39.027230Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Loading The Dataset","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/samsum-dataset-text-summarization/samsum-train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/samsum-dataset-text-summarization/samsum-test.csv\")\nval_data = pd.read_csv(\"/kaggle/input/samsum-dataset-text-summarization/samsum-validation.csv\")\n\n\ntrain_data = Dataset.from_pandas(train_data)\ntest_data = Dataset.from_pandas(test_data)\nval_data = Dataset.from_pandas(val_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T12:21:39.029218Z","iopub.execute_input":"2024-12-17T12:21:39.029734Z","iopub.status.idle":"2024-12-17T12:21:39.441212Z","shell.execute_reply.started":"2024-12-17T12:21:39.029703Z","shell.execute_reply":"2024-12-17T12:21:39.440511Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Loading the Model","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T12:21:39.442194Z","iopub.execute_input":"2024-12-17T12:21:39.442481Z","iopub.status.idle":"2024-12-17T12:21:49.560398Z","shell.execute_reply.started":"2024-12-17T12:21:39.442454Z","shell.execute_reply":"2024-12-17T12:21:49.559705Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dde5be9cf36b4748b917b9599b43715f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0cc6b4275834a1da9d7b0eb16c3dddc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de16b547795c436b959873a33632c79d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"786fbe2b4352431c94ff554382477998"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8faa2ac275d74c2caa94608ae1ef3840"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a924447dfbbe4d28b568043bb764beca"}},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"def preprocess_data(examples):\n    \"\"\"Tokenize the inputs and outputs for training.\"\"\"\n    model_inputs = tokenizer(\n        examples['dialogue'], \n        max_length=512, \n        truncation=True, \n        padding=\"max_length\"  \n    )\n    labels = tokenizer(\n        examples['summary'], \n        max_length=128, \n        truncation=True, \n        padding=\"max_length\" \n    )\n    \n    \n    labels[\"input_ids\"] = [\n        [(label if label != tokenizer.pad_token_id else -100) for label in label_ids] \n        for label_ids in labels[\"input_ids\"]\n    ]\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2024-12-17T12:21:49.561871Z","iopub.execute_input":"2024-12-17T12:21:49.562213Z","iopub.status.idle":"2024-12-17T12:21:49.568098Z","shell.execute_reply.started":"2024-12-17T12:21:49.562175Z","shell.execute_reply":"2024-12-17T12:21:49.566960Z"}}},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"def preprocess_data(examples):\n    \n    \n    dialogues = [str(x) for x in examples['dialogue']]\n    summaries = [str(x) for x in examples['summary']]\n    \n    \n    model_inputs = tokenizer(\n        dialogues,  \n        max_length=512,\n        truncation=True,\n        padding=\"max_length\"\n    )\n    \n   \n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            summaries,  \n            max_length=128,\n            truncation=True,\n            padding=\"max_length\"\n        )\n    \n    \n    model_inputs[\"labels\"] = [\n        [(label if label != tokenizer.pad_token_id else -100) for label in label_ids]\n        for label_ids in labels[\"input_ids\"]\n    ]\n    \n    return model_inputs\n\n\ntry:\n    train_data = train_data.map(preprocess_data, batched=True, remove_columns=[\"dialogue\", \"summary\"])\n    test_data = test_data.map(preprocess_data, batched=True, remove_columns=[\"dialogue\", \"summary\"])\n    val_data = val_data.map(preprocess_data, batched=True, remove_columns=[\"dialogue\", \"summary\"])\nexcept Exception as e:\n    print(f\"Error during preprocessing: {e}\")\n    raise\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T12:22:29.271014Z","iopub.execute_input":"2024-12-17T12:22:29.272011Z","iopub.status.idle":"2024-12-17T12:22:37.589043Z","shell.execute_reply.started":"2024-12-17T12:22:29.271968Z","shell.execute_reply":"2024-12-17T12:22:37.588249Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14732 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcf36fc08b9e4728b1ed49c902efdeb7"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92f707cab5484462ae6bc330fe034b95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adc65ad0e80f43b695ce2477b1085045"}},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,  \n    per_device_eval_batch_size=4,\n    weight_decay=0.01,\n    save_total_limit=2,\n    num_train_epochs=5,\n    predict_with_generate=True,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    report_to=\"wandb\", \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T12:23:32.197090Z","iopub.execute_input":"2024-12-17T12:23:32.197451Z","iopub.status.idle":"2024-12-17T12:23:32.344818Z","shell.execute_reply.started":"2024-12-17T12:23:32.197421Z","shell.execute_reply":"2024-12-17T12:23:32.343858Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=val_data,  \n    tokenizer=tokenizer,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T12:23:32.748246Z","iopub.execute_input":"2024-12-17T12:23:32.748547Z","iopub.status.idle":"2024-12-17T12:23:34.038553Z","shell.execute_reply.started":"2024-12-17T12:23:32.748518Z","shell.execute_reply":"2024-12-17T12:23:34.037631Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/3213395599.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T12:23:34.040044Z","iopub.execute_input":"2024-12-17T12:23:34.040303Z","iopub.status.idle":"2024-12-17T16:11:14.395013Z","shell.execute_reply.started":"2024-12-17T12:23:34.040275Z","shell.execute_reply":"2024-12-17T16:11:14.394341Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241217_122439-2diuqg7s</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/pronaydebnath99-ahsanullah-university-of-science-technology/huggingface/runs/2diuqg7s' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/pronaydebnath99-ahsanullah-university-of-science-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/pronaydebnath99-ahsanullah-university-of-science-technology/huggingface' target=\"_blank\">https://wandb.ai/pronaydebnath99-ahsanullah-university-of-science-technology/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/pronaydebnath99-ahsanullah-university-of-science-technology/huggingface/runs/2diuqg7s' target=\"_blank\">https://wandb.ai/pronaydebnath99-ahsanullah-university-of-science-technology/huggingface/runs/2diuqg7s</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9210' max='9210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9210/9210 3:46:31, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.357400</td>\n      <td>1.362274</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.042500</td>\n      <td>1.394127</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.810100</td>\n      <td>1.478627</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.637900</td>\n      <td>1.588340</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.483200</td>\n      <td>1.687714</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=9210, training_loss=0.8925029466258327, metrics={'train_runtime': 13659.9796, 'train_samples_per_second': 5.392, 'train_steps_per_second': 0.674, 'total_flos': 7.981446249578496e+16, 'train_loss': 0.8925029466258327, 'epoch': 5.0})"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"# Saving the model","metadata":{}},{"cell_type":"code","source":"trainer.save_model(\"./fine_tuned_bart\")\ntokenizer.save_pretrained(\"./fine_tuned_bart\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T16:19:26.003591Z","iopub.execute_input":"2024-12-17T16:19:26.003992Z","iopub.status.idle":"2024-12-17T16:19:30.130106Z","shell.execute_reply.started":"2024-12-17T16:19:26.003959Z","shell.execute_reply":"2024-12-17T16:19:30.129231Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"('./fine_tuned_bart/tokenizer_config.json',\n './fine_tuned_bart/special_tokens_map.json',\n './fine_tuned_bart/vocab.json',\n './fine_tuned_bart/merges.txt',\n './fine_tuned_bart/added_tokens.json',\n './fine_tuned_bart/tokenizer.json')"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"def summarize_text(text):\n    \n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n    model.to(device)\n\n    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n    inputs = {key: value.to(device) for key, value in inputs.items()} \n\n    outputs = model.generate(\n        **inputs,\n        max_length=50, \n        num_beams=6,\n        early_stopping=True\n    )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T16:42:15.889586Z","iopub.execute_input":"2024-12-17T16:42:15.889970Z","iopub.status.idle":"2024-12-17T16:42:15.896189Z","shell.execute_reply.started":"2024-12-17T16:42:15.889939Z","shell.execute_reply":"2024-12-17T16:42:15.895391Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"import torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T16:24:01.035645Z","iopub.execute_input":"2024-12-17T16:24:01.035917Z","iopub.status.idle":"2024-12-17T16:24:01.040243Z","shell.execute_reply.started":"2024-12-17T16:24:01.035892Z","shell.execute_reply":"2024-12-17T16:24:01.039273Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"test_dialogue = \"\"\" Artificial intelligence (AI) has become a transformative force across various industries, enabling advancements in healthcare, finance, and education. By analyzing large datasets, AI can uncover patterns and provide insights that were previously unattainable. For instance, in healthcare, AI-powered tools can assist doctors in diagnosing diseases early, potentially saving lives. Similarly, in finance, AI algorithms help detect fraudulent transactions and optimize investment strategies. While AI offers immense benefits, ethical concerns, such as data privacy and bias, must be addressed to ensure its responsible use.\"\"\"\n\nprint(\"Summary:\", summarize_text(test_dialogue))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T16:42:17.434110Z","iopub.execute_input":"2024-12-17T16:42:17.434426Z","iopub.status.idle":"2024-12-17T16:42:18.362467Z","shell.execute_reply.started":"2024-12-17T16:42:17.434398Z","shell.execute_reply":"2024-12-17T16:42:18.361452Z"}},"outputs":[{"name":"stdout","text":"Summary: Artificial intelligence (AI) has become a transformative force across various industries, enabling advancements in healthcare, finance, and education. Ethical concerns must be addressed to ensure its responsible use, such as data privacy and bias. \n","output_type":"stream"}],"execution_count":34}]}